# project

#### 题目描述

100GB url 文件，使用 1GB 内存计算出出现次数 top100 的 url 和出现的次数。



#### 生成数据

用python脚本生成数据，在这里为了方便我们讲URL的前缀固定为我的github名称 https://github.com/gzgywh/project ，后面加上(0,9999)之间的随机数以区分不同的URL，运行命令如下：

```cc
python size filename
```

有2个可配置的参数。size为我要生成多大的数据，单位是GB(在程序内部做了换算)。filename为我要生成程序的名称，比如我要生成一个大小为10GB，名字为test的URL数据，命令如下：

```cc
python 10 testdata.txt
```



#### 设计思路

因为数据量远远超过内存的大小，所以直接在内存中统计每个URL出现的次数肯定是不行的。所以我们必须将大文件切分成小文件，让有限内存的能够处理数据，然后对小文件分别进行统计后再将结果合并，取得其中出现次数Top100的URL。



#### 具体实现

1.使用EFLHash算法对原始文件中的URL进hash，因为使用的同一个Hash算法，所以相同的URL一定会得到相同的hash值，然后我定义filenum个文件，相当于有filename个剩余类，不同的hash值会把文件对应到不同的剩余类中去

2.统计小文件中每个URL出现的次数，并维护出现次数前TopK个URL以及对应它出现的次数，这里可以先用unordered_map统计一遍每个单词出现的次数，然后用multimap比较一下出现的频次为这个数的URL有哪些，然后出现次数从大到小倒着扫一遍，并把这个结果另外存到一个文件中

3.建立一个以cnt为关键字的优先队列，然后在优先队列中只维护100个元素，如果元素再有新的元素加入，则和优先队列中cnt最小的元素进行比较，如果比堆顶元素的cnt大，则把堆顶出堆，把新元素插入堆中，否则，不做任何改变。这样最后保证存在于优先队列当中的一定是出现次数Top100的URL

4.最后将优先队列里面的元素取出，再按照cnt为关键字进行排序



#### 运行方式

```cc
g++ -std=c++14 main.cpp cut.cpp -o main.out
./main.out
```



#### 实验结果

在这里，我们以10GB大小的数据做测试，生成URL的后缀范围为(1,9999)。同时，我们在控制台输出出现次数前100的url以及前对应的次数，并按照次数从大到小的顺序进行排序。实验结果如下图所示：

![](result.png)



